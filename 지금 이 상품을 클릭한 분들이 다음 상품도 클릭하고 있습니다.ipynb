{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고, todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[todo] - 길이 시각화, 세션 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation 으로 사용하는 함수 (recall, MRR) mAP\n",
    "- Session Based Task 이해\n",
    "- Train/Valid/Test 전략\n",
    "- Session-Parrarel Mini-Batch 를 왜 썼는지 -> 사실 요즘 논문에서는 거의 안쓴다.대신 데이터 특징을 살린 모델링.\n",
    "- (참고) loss, sampling 제외\n",
    "- 라벨을 자체적으로 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [recsys 2015 challenge](https://recsys.yoochoose.net/challenge.html) dataset\n",
    "- (참고) 7z 확장자로 압축되어 있음. 다운로드 및 압축푸는 과정은 생략함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ![aladin](./asset/시크릿모드.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The YOOCHOOSE dataset contain a collection of sessions from a retailer, where each session\n",
    "is encapsulating the click events that the user performed in the session.\n",
    "For some of the sessions, there are also buy events; means that the session ended\n",
    "with the user bought something from the web shop. The data was collected during several\n",
    "months in the year of 2014, reflecting the clicks and purchases performed by the users\n",
    "of an on-line retailer in Europe.  **To protect end users privacy, as well as the retailer,\n",
    "all numbers have been modified.** Do not try to reveal the identity of the retailer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:08:53.895799Z",
     "start_time": "2020-10-17T04:08:53.498835Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "PATH_TO_ORIGINAL_DATA = '/Users/zimin/Downloads/archive/'  # 'D:\\\\data\\\\yoochoose-data\\\\'\n",
    "PATH_TO_PROCESSED_DATA = '/Users/zimin/Downloads/archive/'  # 'D:\\\\data\\\\yoochoose-data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:08:58.483712Z",
     "start_time": "2020-10-17T04:08:57.419283Z"
    },
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train set\n",
      "\tEvents: 70278\n",
      "\tSessions: 17794\n",
      "\tItems: 2933\n",
      "Test set\n",
      "\tEvents: 13568\n",
      "\tSessions: 3416\n",
      "\tItems: 1771\n",
      "Train set\n",
      "\tEvents: 53254\n",
      "\tSessions: 13629\n",
      "\tItems: 2873\n",
      "Validation set\n",
      "\tEvents: 16539\n",
      "\tSessions: 4084\n",
      "\tItems: 2029\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(PATH_TO_ORIGINAL_DATA + 'yoochoose-clicks.dat', sep=',', header=None, usecols=[0, 1, 2],\n",
    "                   parse_dates=[1],\n",
    "                   dtype={0: np.int32, 2: np.int32}, nrows=100000)\n",
    "data.columns = ['SessionId', 'Time', 'ItemId']\n",
    "\n",
    "session_lengths = data.groupby('SessionId').size()\n",
    "data = data[np.in1d(data.SessionId, session_lengths[session_lengths > 1].index)]\n",
    "\n",
    "item_supports = data.groupby('ItemId').size()\n",
    "data = data[np.in1d(data.ItemId, item_supports[item_supports >= 5].index)]\n",
    "\n",
    "session_lengths = data.groupby('SessionId').size()\n",
    "data = data[np.in1d(data.SessionId, session_lengths[session_lengths >= 2].index)]\n",
    "\n",
    "max_time = data['Time'].max()\n",
    "session_max_times = data.groupby('SessionId')['Time'].max()\n",
    "session_train = session_max_times[session_max_times < max_time - dt.timedelta(1)].index\n",
    "session_test = session_max_times[session_max_times >= max_time - dt.timedelta(1)].index\n",
    "\n",
    "train = data[np.in1d(data.SessionId, session_train)]\n",
    "test = data[np.in1d(data.SessionId, session_test)]\n",
    "\n",
    "test = test[np.in1d(test.ItemId, train.ItemId)]\n",
    "\n",
    "test_length = test.groupby('SessionId').size()\n",
    "test = test[np.in1d(test.SessionId, test_length[test_length >= 2].index)]\n",
    "\n",
    "print(\n",
    "    f'Full train set\\n\\tEvents: {len(train)}\\n\\tSessions: {train.SessionId.nunique()}\\n\\tItems: {train.ItemId.nunique()}')\n",
    "train.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_full.txt', sep='\\t', index=False)\n",
    "\n",
    "print(f'Test set\\n\\tEvents: {len(test)}\\n\\tSessions: {test.SessionId.nunique()}\\n\\tItems: {test.ItemId.nunique()}')\n",
    "test.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_test.txt', sep='\\t', index=False)\n",
    "\n",
    "max_train_time = train.Time.max()\n",
    "session_max_times = train.groupby('SessionId').Time.max()\n",
    "session_train = session_max_times[session_max_times < max_train_time - dt.timedelta(1)].index\n",
    "session_valid = session_max_times[session_max_times >= max_train_time - dt.timedelta(1)].index\n",
    "train_tr = train[np.in1d(train.SessionId, session_train)]\n",
    "valid = train[np.in1d(train.SessionId, session_valid)]\n",
    "valid = valid[np.in1d(valid.ItemId, train_tr.ItemId)]\n",
    "valid_length = valid.groupby('SessionId').size()\n",
    "valid = valid[np.in1d(valid.SessionId, valid_length[valid_length >= 2].index)]\n",
    "print(\n",
    "    f'Train set\\n\\tEvents: {len(train_tr)}\\n\\tSessions: {train_tr.SessionId.nunique()}\\n\\tItems: {train_tr.ItemId.nunique()}')\n",
    "train_tr.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_tr.txt', sep='\\t', index=False)\n",
    "\n",
    "print(\n",
    "    f'Validation set\\n\\tEvents: {len(valid)}\\n\\tSessions: {valid.SessionId.nunique()}\\n\\tItems: {valid.ItemId.nunique()}')\n",
    "valid.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_valid.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:08:59.480782Z",
     "start_time": "2020-10-17T04:08:59.474005Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"\n",
    "\n",
    "    def __init__(self, data, session_key='SessionId', item_key='ItemId', time_key='Time'):\n",
    "        self.df = data\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.idx2id = self.add_item_indices()\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "        # clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "        self.click_offsets = self.get_click_offsets()\n",
    "        self.session_idx_arr = np.arange(self.df[self.session_key].nunique())  # indexing to SessionId\n",
    "\n",
    "    def add_item_indices(self):\n",
    "        idx2id = {index: item_id for item_id, index in enumerate(self.df['ItemId'].unique())}\n",
    "        self.df['item_idx'] = self.df['ItemId'].map(idx2id.get)\n",
    "        return idx2id\n",
    "\n",
    "    @property\n",
    "    def items(self):\n",
    "        return self.df['ItemId'].unique()\n",
    "\n",
    "    def get_click_offsets(self):\n",
    "        \"\"\"\n",
    "        Return the offsets of the beginning clicks of each session IDs,\n",
    "        where the offset is calculated against the first click of the first session ID.\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        # group & sort the df by session_key and get the offset values\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "\n",
    "        return offsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:09:00.392110Z",
     "start_time": "2020-10-17T04:09:00.256505Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zimin/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/zimin/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "self = SessionDataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:32:51.882194Z",
     "start_time": "2020-10-17T04:32:51.874428Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SessionDataLoader:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, batch_size=50):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "            dataset (SessionDataset): the session dataset to generate the batches from\n",
    "            batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.done_sessions_counter = 0\n",
    "\n",
    "    def __iter__(self):  # https://dojang.io/mod/page/view.php?id=2405\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,):  Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.dataset.df\n",
    "        self.n_items = df['ItemId'].nunique() + 1\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        max_iter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]  # Session Start\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]  # Session End\n",
    "        mask = []  # indicator for the sessions to be terminated\n",
    "        finished = False\n",
    "\n",
    "        while not finished:\n",
    "            min_len = (end - start).min()  # Shortest Session\n",
    "            # Item indices (for embedding) for clicks where the first sessions start\n",
    "            for i in range(min_len - 1):\n",
    "                # Build inputs & targets\n",
    "                inp = df.item_idx.values[start + i]\n",
    "                target = df.item_idx.values[start + i + 1]\n",
    "                yield inp, target, mask\n",
    "\n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (min_len - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            self.done_sessions_counter = len(mask)\n",
    "            for idx in mask:\n",
    "                max_iter += 1\n",
    "                if max_iter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = max_iter\n",
    "                start[idx] = click_offsets[session_idx_arr[max_iter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[max_iter] + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:45:03.553211Z",
     "start_time": "2020-10-17T04:45:03.550362Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "two = SessionDataLoader(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 구조가 간단하므로 Funtional 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T05:10:59.510221Z",
     "start_time": "2020-10-17T05:10:56.909554Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GRU\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:31:14.261736Z",
     "start_time": "2020-10-17T06:31:14.257500Z"
    }
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, num_items, batch_size, hsz, drop_rate, lr, epochs):\n",
    "        self.num_items = num_items\n",
    "        self.batch_size = batch_size\n",
    "        self.hsz = hsz\n",
    "        self.drop_rate = drop_rate\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "args = Args(2934, 20, 50, 0.1, 0.001, 3)\n",
    "args.train_samples_qty = train['SessionId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:31:14.931886Z",
     "start_time": "2020-10-17T06:31:14.760198Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(20, 1, 2934)]           0         \n",
      "_________________________________________________________________\n",
      "GRU (GRU)                    [(20, 50), (20, 50)]      447900    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (20, 50)                  0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (20, 2934)                149634    \n",
      "=================================================================\n",
      "Total params: 597,534\n",
      "Trainable params: 597,534\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(batch_shape=(args.batch_size, 1, args.num_items))\n",
    "gru, gru_states = GRU(args.hsz, stateful=True, return_state=True, name='GRU')(inputs)\n",
    "dropout = Dropout(args.drop_rate)(gru)\n",
    "predictions = Dense(args.num_items, activation='softmax')(dropout)\n",
    "model = Model(inputs=inputs, outputs=[predictions])\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(0.001))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:31:14.944936Z",
     "start_time": "2020-10-17T06:31:14.940504Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, args):\n",
    "    train_dataset = SessionDataset(train)\n",
    "    train_loader = SessionDataLoader(train_dataset, batch_size=args.batch_size)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loader = tqdm(train_loader, total=args.train_samples_qty)\n",
    "        for i, (feat, target, mask) in enumerate(loader):\n",
    "            reset_hidden_states(model, mask)\n",
    "\n",
    "            input_ohe = to_categorical(feat, num_classes=train_loader.n_items)\n",
    "            input_ohe = np.expand_dims(input_ohe, axis=1)\n",
    "            target_ohe = to_categorical(target, num_classes=train_loader.n_items)\n",
    "\n",
    "            tr_loss = model.train_on_batch(input_ohe, target_ohe)\n",
    "            loader.set_postfix(train_loss=tr_loss)\n",
    "\n",
    "\n",
    "def reset_hidden_states(model, mask):\n",
    "    gru_layer = model.get_layer(name='GRU')\n",
    "    hidden_states = gru_layer.states[0].numpy()\n",
    "    for elt in mask:\n",
    "        hidden_states[elt, :] = 0\n",
    "    gru_layer.reset_states(states=hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:31:32.589547Z",
     "start_time": "2020-10-17T06:31:15.394998Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zimin/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/zimin/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      " 12%|█▏        | 2111/17794 [00:17<02:06, 124.04it/s, train_loss=6.47]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-feeb9149d92b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-358f0d595075>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtarget_ohe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ohe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ohe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1691\u001b[0m       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n\u001b[1;32m   1692\u001b[0m                                                     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1693\u001b[0;31m                                                     class_weight)\n\u001b[0m\u001b[1;32m   1694\u001b[0m       \u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1532\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_distribute_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec, job_token)\u001b[0m\n\u001b[1;32m    694\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    700\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mautotune\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m       \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ModelDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu_budget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;31m# (4) Apply stats aggregator options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, algorithm, cpu_budget)\u001b[0m\n\u001b[1;32m   4379\u001b[0m         \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4380\u001b[0m         \u001b[0mcpu_budget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu_budget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4381\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   4382\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ModelDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmodel_dataset\u001b[0;34m(input_dataset, output_types, output_shapes, algorithm, cpu_budget, name)\u001b[0m\n\u001b[1;32m   3289\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"algorithm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cpu_budget\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3290\u001b[0m         \u001b[0mcpu_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3291\u001b[0;31m         output_shapes)\n\u001b[0m\u001b[1;32m   3292\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SeqRecSys",
   "language": "python",
   "name": "seqrecsys"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
