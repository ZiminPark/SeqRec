{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고, todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[todo] - 길이 시각화, 세션 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation 으로 사용하는 함수 (recall, MRR) mAP\n",
    "- Session Based Task 이해\n",
    "- Train/Valid/Test 전략\n",
    "- Session-Parrarel Mini-Batch 를 왜 썼는지 -> 사실 요즘 논문에서는 거의 안쓴다.대신 데이터 특징을 살린 모델링.\n",
    "- (참고) loss, sampling 제외"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [recsys 2015 challenge](https://recsys.yoochoose.net/challenge.html) dataset\n",
    "- (참고) 7z 확장자로 압축되어 있음. 다운로드 및 압축푸는 과정은 생략함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ![aladin](./asset/시크릿모드.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The YOOCHOOSE dataset contain a collection of sessions from a retailer, where each session\n",
    "is encapsulating the click events that the user performed in the session.\n",
    "For some of the sessions, there are also buy events; means that the session ended\n",
    "with the user bought something from the web shop. The data was collected during several\n",
    "months in the year of 2014, reflecting the clicks and purchases performed by the users\n",
    "of an on-line retailer in Europe.  **To protect end users privacy, as well as the retailer,\n",
    "all numbers have been modified.** Do not try to reveal the identity of the retailer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:08:53.895799Z",
     "start_time": "2020-10-17T04:08:53.498835Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "PATH_TO_ORIGINAL_DATA = '/Users/zimin/Downloads/archive/'  # 'D:\\\\data\\\\yoochoose-data\\\\'\n",
    "PATH_TO_PROCESSED_DATA = '/Users/zimin/Downloads/archive/'  # 'D:\\\\data\\\\yoochoose-data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:08:58.483712Z",
     "start_time": "2020-10-17T04:08:57.419283Z"
    },
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train set\n",
      "\tEvents: 70278\n",
      "\tSessions: 17794\n",
      "\tItems: 2933\n",
      "Test set\n",
      "\tEvents: 13568\n",
      "\tSessions: 3416\n",
      "\tItems: 1771\n",
      "Train set\n",
      "\tEvents: 53254\n",
      "\tSessions: 13629\n",
      "\tItems: 2873\n",
      "Validation set\n",
      "\tEvents: 16539\n",
      "\tSessions: 4084\n",
      "\tItems: 2029\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(PATH_TO_ORIGINAL_DATA + 'yoochoose-clicks.dat', sep=',', header=None, usecols=[0, 1, 2],\n",
    "                   parse_dates=[1],\n",
    "                   dtype={0: np.int32, 2: np.int32}, nrows=100000)\n",
    "data.columns = ['SessionId', 'Time', 'ItemId']\n",
    "\n",
    "session_lengths = data.groupby('SessionId').size()\n",
    "data = data[np.in1d(data.SessionId, session_lengths[session_lengths > 1].index)]\n",
    "\n",
    "item_supports = data.groupby('ItemId').size()\n",
    "data = data[np.in1d(data.ItemId, item_supports[item_supports >= 5].index)]\n",
    "\n",
    "session_lengths = data.groupby('SessionId').size()\n",
    "data = data[np.in1d(data.SessionId, session_lengths[session_lengths >= 2].index)]\n",
    "\n",
    "max_time = data['Time'].max()\n",
    "session_max_times = data.groupby('SessionId')['Time'].max()\n",
    "session_train = session_max_times[session_max_times < max_time - dt.timedelta(1)].index\n",
    "session_test = session_max_times[session_max_times >= max_time - dt.timedelta(1)].index\n",
    "\n",
    "train = data[np.in1d(data.SessionId, session_train)]\n",
    "test = data[np.in1d(data.SessionId, session_test)]\n",
    "\n",
    "test = test[np.in1d(test.ItemId, train.ItemId)]\n",
    "\n",
    "test_length = test.groupby('SessionId').size()\n",
    "test = test[np.in1d(test.SessionId, test_length[test_length >= 2].index)]\n",
    "\n",
    "print(\n",
    "    f'Full train set\\n\\tEvents: {len(train)}\\n\\tSessions: {train.SessionId.nunique()}\\n\\tItems: {train.ItemId.nunique()}')\n",
    "train.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_full.txt', sep='\\t', index=False)\n",
    "\n",
    "print(f'Test set\\n\\tEvents: {len(test)}\\n\\tSessions: {test.SessionId.nunique()}\\n\\tItems: {test.ItemId.nunique()}')\n",
    "test.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_test.txt', sep='\\t', index=False)\n",
    "\n",
    "max_train_time = train.Time.max()\n",
    "session_max_times = train.groupby('SessionId').Time.max()\n",
    "session_train = session_max_times[session_max_times < max_train_time - dt.timedelta(1)].index\n",
    "session_valid = session_max_times[session_max_times >= max_train_time - dt.timedelta(1)].index\n",
    "train_tr = train[np.in1d(train.SessionId, session_train)]\n",
    "valid = train[np.in1d(train.SessionId, session_valid)]\n",
    "valid = valid[np.in1d(valid.ItemId, train_tr.ItemId)]\n",
    "valid_length = valid.groupby('SessionId').size()\n",
    "valid = valid[np.in1d(valid.SessionId, valid_length[valid_length >= 2].index)]\n",
    "print(\n",
    "    f'Train set\\n\\tEvents: {len(train_tr)}\\n\\tSessions: {train_tr.SessionId.nunique()}\\n\\tItems: {train_tr.ItemId.nunique()}')\n",
    "train_tr.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_tr.txt', sep='\\t', index=False)\n",
    "\n",
    "print(\n",
    "    f'Validation set\\n\\tEvents: {len(valid)}\\n\\tSessions: {valid.SessionId.nunique()}\\n\\tItems: {valid.ItemId.nunique()}')\n",
    "valid.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_valid.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:08:59.480782Z",
     "start_time": "2020-10-17T04:08:59.474005Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"\n",
    "\n",
    "    def __init__(self, data, session_key='SessionId', item_key='ItemId', time_key='Time'):\n",
    "        self.df = data\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.idx2id = self.add_item_indices()\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "        # clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "        self.click_offsets = self.get_click_offsets()\n",
    "        self.session_idx_arr = np.arange(self.df[self.session_key].nunique())  # indexing to SessionId\n",
    "\n",
    "    def add_item_indices(self):\n",
    "        idx2id = {index: item_id for item_id, index in enumerate(self.df['ItemId'].unique())}\n",
    "        self.df['item_idx'] = self.df['ItemId'].map(idx2id.get)\n",
    "        return idx2id\n",
    "\n",
    "    @property\n",
    "    def items(self):\n",
    "        return self.df['ItemId'].unique()\n",
    "\n",
    "    def get_click_offsets(self):\n",
    "        \"\"\"\n",
    "        Return the offsets of the beginning clicks of each session IDs,\n",
    "        where the offset is calculated against the first click of the first session ID.\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        # group & sort the df by session_key and get the offset values\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "\n",
    "        return offsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:09:00.392110Z",
     "start_time": "2020-10-17T04:09:00.256505Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zimin/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/zimin/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "self = SessionDataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:20:20.515465Z",
     "start_time": "2020-10-17T04:20:20.507755Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SessionDataLoader:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, batch_size=50):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "            dataset (SessionDataset): the session dataset to generate the batches from\n",
    "            batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.done_sessions_counter = 0\n",
    "\n",
    "    def __iter__(self):  # https://dojang.io/mod/page/view.php?id=2405\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,):  Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.dataset.df\n",
    "        self.n_items = df['ItemId'].nunique() + 1\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        max_iter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]  # Session Start\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]  # Session End\n",
    "        mask = []  # indicator for the sessions to be terminated\n",
    "        finished = False\n",
    "\n",
    "        while not finished:\n",
    "            min_len = (end - start).min()  # Shortest Session\n",
    "            # Item indices (for embedding) for clicks where the first sessions start\n",
    "            idx_start = df.item_idx.values[start]\n",
    "            print(f'minlen{min_len}')\n",
    "            for i in range(min_len - 1):\n",
    "                # Build inputs & targets\n",
    "                inp = idx_start\n",
    "                target = df.item_idx.values[start + i + 1]\n",
    "                print('yield')\n",
    "                yield inp, target, mask\n",
    "            print('out')\n",
    "\n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (min_len - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            self.done_sessions_counter = len(mask)\n",
    "            for idx in mask:\n",
    "                max_iter += 1\n",
    "                if max_iter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = max_iter\n",
    "                start[idx] = click_offsets[session_idx_arr[max_iter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[max_iter] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:20:21.116779Z",
     "start_time": "2020-10-17T04:20:21.108275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minlen2\n",
      "yield\n",
      "[  0   2   4   6   7   8  16  17  19  23  21  19  25  24  36  38  44  42\n",
      "  45  46  52  54  50  58  55  56  59  60  63  70  67  76  75  73  72  90\n",
      "  81  79  94  58  97  91  71 111 113 114  99 102 105 109] [  1   3   5   6   7   8  16  18  20  23  22  19  25  24  36  39  44  43\n",
      "  45  47  53  45  51  58  55  57  59  61  64  71  68  77  75  74  72  90\n",
      "  41  80  95  58  98  92  93 111 113 115 100 102 106 110] []\n",
      "out\n",
      "minlen2\n",
      "yield\n",
      "[ 23 116 123 125   7   8 118  18  20  23 121 130  25  24  36  39 131 129\n",
      " 127  47  98 141   8  58 135 137  59  61  64 133  68  77 153 150 149  90\n",
      "  41  50  95  58 146 144 164 111 162 157 100 102 106 113] [117 116 124 126   7   9 119  17  19  23 122 130  25  24  37  40 132 129\n",
      " 128  47  98 142 143  58 136 137  59  62  65 134  68  77 154 151   8  90\n",
      "  82 137  96  58 147 145 165 112 163 157 101 102 105 113] [ 0  1  2  3  6 10 11 16 17 18 20 21 22 24 25 29 32 33 34 37 40 41 42 44\n",
      " 45 49]\n",
      "out\n",
      "minlen2\n",
      "yield\n",
      "[156 155   3 130 151   9 119 182 181 184  96 171  25  24  37  40 174  22\n",
      " 177  47 146 194  23  58 136 137  59 187  65 181  68  77 192 151 193  90\n",
      "  82 137 194 199  90 213 165 112 219 157 217 102 105 206] [156 155 108 130 180   9 120 183 181 185 173 171  26  24  37  41 175   3\n",
      " 177  47 201 198  23  58 136 138  59 187  66 190  68  78 192 152  31  90\n",
      "  83 148 195 200  48 214 166 112 220 157 218 103 105 207] [ 0  1  2  3  4  7  8  9 10 11 16 17 18 20 21 22 27 29 32 34 38 39 40 41\n",
      " 44 46 49]\n",
      "out\n",
      "minlen2\n",
      "yield\n",
      "[156 141 203 130 180   9 210 156 208 185 232 171  26   0  92 235 175 234\n",
      " 177  47 201 221  23  58 223 138  59 187 118 190  68 228 110 198 252  90\n",
      "  83 148 195 245 243 214 166 247 220 157 237 103 105 207] [156 205 204 178 109   9 211 209  23 186 233 172  26  59 111 235 176 234\n",
      " 177  48 196 222  23  58 224 138  59 188 120 190  68 228 249 198 252  90\n",
      "  84 109 195 245 243 215 167 248  21 158 238 103 105 207] [ 1  2  6  7  8 10 13 14 15 17 21 24 28 31 32 33 34 39 40 43 46]\n",
      "out\n",
      "minlen2\n",
      "yield\n",
      "[242 239 204 178 236   9 211 196 274 186 264 172  26  59 111  58 176 255\n",
      " 262  48 196 146 285 248 224 138  59 188 179 190  68 228 249 283 252  28\n",
      "  84 120 195 245 243 215 167 248  21 158 156 103 105 280] [242 240  20  99 236  10 212 273 268 184 264 172  27  59 230  58 176 256\n",
      " 263  48 202 253 111 284 223 139  59 189 149 191  69 229 250 281 252 282\n",
      "  85 119 196 246 244 216 164 129 112 159 156 104 107 121] [ 0  1  4  7  8 10 15 17 18 21 22 23 28 33 35 37 46 49]\n",
      "out\n",
      "minlen2\n",
      "yield\n",
      "[118 240 276  99 298  10 295 273 268 292 264 291  27 121 230 288 176 256\n",
      "  58  48 323 253 111 327 223 139 331  18 334 191 156 338 250 281 252 336\n",
      "  85 302 196 246 244 299 164 311 112 159 156 104 107 306] [279  17 277   8 298  11 296  24 275 293 265 288  28 178 231 288 175 257\n",
      " 264  48 324 254 285  70 225 139 332  18 335  71 337 276 251 156 252 113\n",
      "  86  21 197 245 156 193 168 312   7 160 281   5 108 306] [ 0  2  4  6  9 11 13 15 18 20 23 26 27 28 30 31 35 37 41 43 49]\n",
      "[118 240 276  99 298  10 295 273 268 292 264 291  27 121 230 288 176 256\n",
      "  58  48 323 253 111 327 223 139 331  18 334 191 156 338 250 281 252 336\n",
      "  85 302 196 246 244 299 164 311 112 159 156 104 107 306] [279  17 277   8 298  11 296  24 275 293 265 288  28 178 231 288 175 257\n",
      " 264  48 324 254 285  70 225 139 332  18 335  71 337 276 251 156 252 113\n",
      "  86  21 197 245 156 193 168 312   7 160 281   5 108 306] [ 0  2  4  6  9 11 13 15 18 20 23 26 27 28 30 31 35 37 41 43 49]\n"
     ]
    }
   ],
   "source": [
    "two = SessionDataLoader(self)\n",
    "\n",
    "count = 0\n",
    "for a,b,c in two:\n",
    "    count += 1\n",
    "    print(a,b,c)\n",
    "    if count == 6:\n",
    "        print(a,b,c)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SeqRecSys",
   "language": "python",
   "name": "seqrecsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
