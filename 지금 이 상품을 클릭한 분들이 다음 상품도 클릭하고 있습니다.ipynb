{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고, todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[todo] - 길이 시각화, 세션 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation 으로 사용하는 함수 (recall, MRR) mAP\n",
    "- Session Based Task 이해\n",
    "- Train/Valid/Test 전략\n",
    "- Session-Parrarel Mini-Batch 를 왜 썼는지 -> 사실 요즘 논문에서는 거의 안쓴다.대신 데이터 특징을 살린 모델링.\n",
    "- (참고) loss, sampling 제외\n",
    "- 라벨을 자체적으로 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [recsys 2015 challenge](https://recsys.yoochoose.net/challenge.html) dataset\n",
    "- (참고) 7z 확장자로 압축되어 있음. 다운로드 및 압축푸는 과정은 생략함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ![aladin](./asset/시크릿모드.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The YOOCHOOSE dataset contain a collection of sessions from a retailer, where each session\n",
    "is encapsulating the click events that the user performed in the session.\n",
    "For some of the sessions, there are also buy events; means that the session ended\n",
    "with the user bought something from the web shop. The data was collected during several\n",
    "months in the year of 2014, reflecting the clicks and purchases performed by the users\n",
    "of an on-line retailer in Europe.  **To protect end users privacy, as well as the retailer,\n",
    "all numbers have been modified.** Do not try to reveal the identity of the retailer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:08:53.895799Z",
     "start_time": "2020-10-17T04:08:53.498835Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "PATH_TO_ORIGINAL_DATA = '/Users/zimin/Downloads/archive/'  # 'D:\\\\data\\\\yoochoose-data\\\\'\n",
    "PATH_TO_PROCESSED_DATA = '/Users/zimin/Downloads/archive/'  # 'D:\\\\data\\\\yoochoose-data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:08:58.483712Z",
     "start_time": "2020-10-17T04:08:57.419283Z"
    },
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train set\n",
      "\tEvents: 70278\n",
      "\tSessions: 17794\n",
      "\tItems: 2933\n",
      "Test set\n",
      "\tEvents: 13568\n",
      "\tSessions: 3416\n",
      "\tItems: 1771\n",
      "Train set\n",
      "\tEvents: 53254\n",
      "\tSessions: 13629\n",
      "\tItems: 2873\n",
      "Validation set\n",
      "\tEvents: 16539\n",
      "\tSessions: 4084\n",
      "\tItems: 2029\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(PATH_TO_ORIGINAL_DATA + 'yoochoose-clicks.dat', sep=',', header=None, usecols=[0, 1, 2],\n",
    "                   parse_dates=[1],\n",
    "                   dtype={0: np.int32, 2: np.int32}, nrows=100000)\n",
    "data.columns = ['SessionId', 'Time', 'ItemId']\n",
    "\n",
    "session_lengths = data.groupby('SessionId').size()\n",
    "data = data[np.in1d(data.SessionId, session_lengths[session_lengths > 1].index)]\n",
    "\n",
    "item_supports = data.groupby('ItemId').size()\n",
    "data = data[np.in1d(data.ItemId, item_supports[item_supports >= 5].index)]\n",
    "\n",
    "session_lengths = data.groupby('SessionId').size()\n",
    "data = data[np.in1d(data.SessionId, session_lengths[session_lengths >= 2].index)]\n",
    "\n",
    "max_time = data['Time'].max()\n",
    "session_max_times = data.groupby('SessionId')['Time'].max()\n",
    "session_train = session_max_times[session_max_times < max_time - dt.timedelta(1)].index\n",
    "session_test = session_max_times[session_max_times >= max_time - dt.timedelta(1)].index\n",
    "\n",
    "train = data[np.in1d(data.SessionId, session_train)]\n",
    "test = data[np.in1d(data.SessionId, session_test)]\n",
    "\n",
    "test = test[np.in1d(test.ItemId, train.ItemId)]\n",
    "\n",
    "test_length = test.groupby('SessionId').size()\n",
    "test = test[np.in1d(test.SessionId, test_length[test_length >= 2].index)]\n",
    "\n",
    "print(\n",
    "    f'Full train set\\n\\tEvents: {len(train)}\\n\\tSessions: {train.SessionId.nunique()}\\n\\tItems: {train.ItemId.nunique()}')\n",
    "train.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_full.txt', sep='\\t', index=False)\n",
    "\n",
    "print(f'Test set\\n\\tEvents: {len(test)}\\n\\tSessions: {test.SessionId.nunique()}\\n\\tItems: {test.ItemId.nunique()}')\n",
    "test.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_test.txt', sep='\\t', index=False)\n",
    "\n",
    "max_train_time = train.Time.max()\n",
    "session_max_times = train.groupby('SessionId').Time.max()\n",
    "session_train = session_max_times[session_max_times < max_train_time - dt.timedelta(1)].index\n",
    "session_valid = session_max_times[session_max_times >= max_train_time - dt.timedelta(1)].index\n",
    "train_tr = train[np.in1d(train.SessionId, session_train)]\n",
    "valid = train[np.in1d(train.SessionId, session_valid)]\n",
    "valid = valid[np.in1d(valid.ItemId, train_tr.ItemId)]\n",
    "valid_length = valid.groupby('SessionId').size()\n",
    "valid = valid[np.in1d(valid.SessionId, valid_length[valid_length >= 2].index)]\n",
    "print(\n",
    "    f'Train set\\n\\tEvents: {len(train_tr)}\\n\\tSessions: {train_tr.SessionId.nunique()}\\n\\tItems: {train_tr.ItemId.nunique()}')\n",
    "train_tr.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_tr.txt', sep='\\t', index=False)\n",
    "\n",
    "print(\n",
    "    f'Validation set\\n\\tEvents: {len(valid)}\\n\\tSessions: {valid.SessionId.nunique()}\\n\\tItems: {valid.ItemId.nunique()}')\n",
    "valid.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_valid.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:08:59.480782Z",
     "start_time": "2020-10-17T04:08:59.474005Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"\n",
    "\n",
    "    def __init__(self, data, session_key='SessionId', item_key='ItemId', time_key='Time'):\n",
    "        self.df = data\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.idx2id = self.add_item_indices()\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "        # clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "        self.click_offsets = self.get_click_offsets()\n",
    "        self.session_idx_arr = np.arange(self.df[self.session_key].nunique())  # indexing to SessionId\n",
    "\n",
    "    def add_item_indices(self):\n",
    "        idx2id = {index: item_id for item_id, index in enumerate(self.df['ItemId'].unique())}\n",
    "        self.df['item_idx'] = self.df['ItemId'].map(idx2id.get)\n",
    "        return idx2id\n",
    "\n",
    "    @property\n",
    "    def items(self):\n",
    "        return self.df['ItemId'].unique()\n",
    "\n",
    "    def get_click_offsets(self):\n",
    "        \"\"\"\n",
    "        Return the offsets of the beginning clicks of each session IDs,\n",
    "        where the offset is calculated against the first click of the first session ID.\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        # group & sort the df by session_key and get the offset values\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "\n",
    "        return offsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:09:00.392110Z",
     "start_time": "2020-10-17T04:09:00.256505Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zimin/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/zimin/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "self = SessionDataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:32:51.882194Z",
     "start_time": "2020-10-17T04:32:51.874428Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SessionDataLoader:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, batch_size=50):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "            dataset (SessionDataset): the session dataset to generate the batches from\n",
    "            batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.done_sessions_counter = 0\n",
    "\n",
    "    def __iter__(self):  # https://dojang.io/mod/page/view.php?id=2405\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,):  Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.dataset.df\n",
    "        self.n_items = df['ItemId'].nunique() + 1\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        max_iter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]  # Session Start\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]  # Session End\n",
    "        mask = []  # indicator for the sessions to be terminated\n",
    "        finished = False\n",
    "\n",
    "        while not finished:\n",
    "            min_len = (end - start).min()  # Shortest Session\n",
    "            # Item indices (for embedding) for clicks where the first sessions start\n",
    "            for i in range(min_len - 1):\n",
    "                # Build inputs & targets\n",
    "                inp = df.item_idx.values[start + i]\n",
    "                target = df.item_idx.values[start + i + 1]\n",
    "                yield inp, target, mask\n",
    "\n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (min_len - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            self.done_sessions_counter = len(mask)\n",
    "            for idx in mask:\n",
    "                max_iter += 1\n",
    "                if max_iter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = max_iter\n",
    "                start[idx] = click_offsets[session_idx_arr[max_iter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[max_iter] + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T04:45:03.553211Z",
     "start_time": "2020-10-17T04:45:03.550362Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "two = SessionDataLoader(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 구조가 간단하므로 Funtional 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T05:10:59.510221Z",
     "start_time": "2020-10-17T05:10:56.909554Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GRU\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T05:45:36.090526Z",
     "start_time": "2020-10-17T05:45:36.086559Z"
    }
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, num_items, batch_size, hsz, drop_rate, lr, epochs):\n",
    "        self.num_items = num_items\n",
    "        self.batch_size = batch_size\n",
    "        self.hsz = hsz\n",
    "        self.drop_rate = drop_rate\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "args = Args(2934, 20, 50, 0.1, 0.001, 3)\n",
    "args.train_samples_qty = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T05:45:36.654505Z",
     "start_time": "2020-10-17T05:45:36.468744Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(20, 1, 2934)]           0         \n",
      "_________________________________________________________________\n",
      "GRU (GRU)                    [(20, 50), (20, 50)]      447900    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (20, 50)                  0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (20, 2934)                149634    \n",
      "=================================================================\n",
      "Total params: 597,534\n",
      "Trainable params: 597,534\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(batch_shape=(args.batch_size, 1, args.num_items))\n",
    "gru, gru_states = GRU(args.hsz, stateful=True, return_state=True, name='GRU')(inputs)\n",
    "dropout = Dropout(args.drop_rate)(gru)\n",
    "predictions = Dense(args.num_items, activation='softmax')(dropout)\n",
    "model = Model(inputs=inputs, outputs=[predictions])\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(0.001))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T05:45:36.835187Z",
     "start_time": "2020-10-17T05:45:36.829431Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, args):\n",
    "    for epoch in range(1, args.epochs):\n",
    "        with tqdm(total=args.train_samples_qty) as pbar:\n",
    "            train_dataset = SessionDataset(train)\n",
    "            loader = SessionDataLoader(train_dataset, batch_size=args.batch_size)\n",
    "            for feat, target, mask in loader:\n",
    "                reset_hidden_states(model, mask)\n",
    "\n",
    "                input_ohe = to_categorical(feat, num_classes=loader.n_items)\n",
    "                input_ohe = np.expand_dims(input_ohe, axis=1)\n",
    "                target_ohe = to_categorical(target, num_classes=loader.n_items)\n",
    "                \n",
    "                tr_loss = model.train_on_batch(input_ohe, target_ohe)\n",
    "\n",
    "                pbar.set_description(f'Epoch {epoch}. Loss: {tr_loss:.5f}')\n",
    "                pbar.update(loader.done_sessions_counter)\n",
    "\n",
    "\n",
    "def reset_hidden_states(model, mask):\n",
    "    gru_layer = model.get_layer(name='GRU')\n",
    "    hidden_states = gru_layer.states[0].numpy()\n",
    "    for elt in mask:\n",
    "        hidden_states[elt, :] = 0\n",
    "    gru_layer.reset_states(states=hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T05:46:23.571472Z",
     "start_time": "2020-10-17T05:45:37.298264Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/Users/zimin/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/zimin/opt/anaconda3/envs/SeqRecSys/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "Epoch 1. Loss: 7.26403: : 17773it [00:23, 752.93it/s]                  \n",
      "Epoch 2. Loss: 6.41864: : 17773it [00:22, 784.24it/s]                   \n"
     ]
    }
   ],
   "source": [
    "train_model(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SeqRecSys",
   "language": "python",
   "name": "seqrecsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
